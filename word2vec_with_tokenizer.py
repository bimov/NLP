import numpy as np
from collections import defaultdict
from logisticregression import LogisticRegression


def get_ngrams(text, n):
    """–§–æ—Ä–º–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö n-–≥—Ä–∞–º–º –≤ —Ç–µ–∫—Å—Ç–µ"""
    ngrams = []
    for word in text.split():
        for i in range(len(word) - n + 1):
            ngrams.append(word[i:i + n])
    return ngrams


def train_tokenizer(text, num_merges=10000, max_ngram=8):
    """–û–±—É—á–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –∑–∞–º–µ–Ω—è—è —á–∞—Å—Ç—ã–µ n-–≥—Ä–∞–º–º—ã —Ç–æ–∫–µ–Ω–∞–º–∏"""
    vocab = {word: 1 for word in text.split()}  # –ù–∞—á–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
    token_map = {}  # –ö–∞—Ä—Ç–∞ –∑–∞–º–µ–Ω —Ç–æ–∫–µ–Ω–æ–≤

    for _ in range(num_merges):
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —á–∞—Å—Ç–æ—Ç n-–≥—Ä–∞–º–º
        ngram_counts = defaultdict(int)
        for n in range(2, max_ngram + 1):  # –ë–µ—Ä–µ–º n-–≥—Ä–∞–º–º—ã —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã
            for word in vocab:
                for ngram in get_ngrams(word, n):
                    ngram_counts[ngram] += vocab[word]

        if not ngram_counts:
            break

        # –í—ã–±–∏—Ä–∞–µ–º —Å–∞–º—É—é —á–∞—Å—Ç—É—é n-–≥—Ä–∞–º–º—É
        best_ngram = max(ngram_counts, key=ngram_counts.get)
        token = f"[{best_ngram}]"  # –ù–æ–≤—ã–π —Ç–æ–∫–µ–Ω (–º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª)

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ª–æ–≤–∞—Ä—å, –∑–∞–º–µ–Ω—è—è n-–≥—Ä–∞–º–º—É –Ω–æ–≤—ã–º —Ç–æ–∫–µ–Ω–æ–º
        new_vocab = {}
        for word, freq in vocab.items():
            new_word = word.replace(best_ngram, token)
            new_vocab[new_word] = freq

        vocab = new_vocab
        token_map[token] = best_ngram  # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç–æ–∫–µ–Ω–∞ n-–≥—Ä–∞–º–º–µ
        if _ % 1 == 0:
            print(_)

    return vocab, token_map


def tokenize(text, token_map):
    """–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç, –∏—Å–ø–æ–ª—å–∑—É—è –æ–±—É—á–µ–Ω–Ω—ã–µ n-–≥—Ä–∞–º–º—ã"""
    tokens = []
    words = text.split()

    for word in words:
        for token, ngram in sorted(token_map.items(), key=lambda x: -len(x[1])):
            word = word.replace(ngram, token)
        tokens.append(word)

    return tokens


class Word2Vec:
    def __init__(self, data, embedding=None, embedding_size=1000, window_size=5, num_merges=10000, max_ngram=8):
        self.raw_data = data
        self.window_size = window_size
        self.embedding_size = embedding_size
        if embedding is None:
            self.embedding = {}
        else:
            self.embedding = embedding
        self.unique = {}
        self.neg_table = None

        # üîπ –û–±—É—á–∞–µ–º n-–≥—Ä–∞–º–º–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        self.vocab, self.token_map = train_tokenizer(" ".join(data), num_merges, max_ngram)
        self.data = self.tokenize_data(data)

    def tokenize_data(self, data):
        """üîπ –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç –≤–µ—Å—å –∫–æ—Ä–ø—É—Å –¥–∞–Ω–Ω—ã—Ö"""
        return [tokenize(sentence, self.token_map) for sentence in data]

    def find_positive_context(self, index):
        """–ù–∞—Ö–æ–¥–∏—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–æ–∫–Ω–æ –≤–æ–∫—Ä—É–≥ —Å–ª–æ–≤–∞)"""
        positive_context = {}
        for i in range(max(0, index - self.window_size), min(index + self.window_size + 1, len(self.data))):
            if i != index:
                positive_context[self.data[i]] = self.embedding[self.data[i]]
        return positive_context

    def find_negative_context(self, index, positive_context):
        """üîπ –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π —Å—ç–º–ø–ª–∏–Ω–≥"""
        counter = 0
        negative_context = {}
        k = self.unique[self.data[index]] // self.window_size
        while counter < self.window_size * k:
            neg = np.random.choice(self.neg_table)
            if self.data[neg] not in positive_context.keys() and neg != index:
                negative_context[self.data[neg]] = self.embedding[self.data[neg]]
                counter += 1
        return negative_context

    def count_unique_word(self):
        """üîπ –ü–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç—ã —Ç–æ–∫–µ–Ω–æ–≤"""
        for word in self.data:
            if word in self.unique:
                self.unique[word] += 1
            else:
                self.unique[word] = 1

        vocab = list(self.unique.keys())
        frequencies = np.array([self.unique[word] for word in vocab], dtype=np.float32)
        probabilities = frequencies ** 0.75
        probabilities /= probabilities.sum()
        self.neg_table = np.random.choice(len(vocab), size=1000000, p=probabilities)

    def make_random_embedding(self):
        """üîπ –°–æ–∑–¥–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        for word in set(sum(self.data, [])):  # –¢–µ–ø–µ—Ä—å –¥–∞–Ω–Ω—ã–µ –≤ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
            if word not in self.embedding:
                self.embedding[word] = np.random.randn(self.embedding_size)

    def learn_embedding(self):
        """üîπ –û–±—É—á–µ–Ω–∏–µ word2vec"""
        self.count_unique_word()
        self.make_random_embedding()

        for i in range(len(self.data)):
            if i % 1000 == 0:
                print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i} / {len(self.data)}")

            pos = self.find_positive_context(i)
            neg = self.find_negative_context(i, pos)
            new_pos, new_neg, new_w = LogisticRegression().logistic_regression(pos, neg, self.embedding[self.data[i]])

            self.embedding[self.data[i]] = new_w
            for word in new_pos:
                self.embedding[word] = new_pos[word]
            for word in new_neg:
                self.embedding[word] = new_neg[word]
